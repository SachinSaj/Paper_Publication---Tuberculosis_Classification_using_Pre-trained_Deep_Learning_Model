{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuberculosis Classification Using different types of Transfer Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Sheshu Babu, Sachin Saj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The task is to classify, whether the given chest X-Ray has tuberculosis or not.\n",
    "\n",
    "- The dataset, which has been used for our task is Montogomery County X-Ray dataset. Preprocessing steps like data-augmentation and histogram equalizations is done to improve their performances.\n",
    "\n",
    "- The pre-trained models used were MobileNetV2 and DenseNet121 (** In this notebook, only mobilenetV2 is shown). The reason for choosing this models were because of their light weight architecture as a result computation time were be reduced\n",
    "\n",
    "- Three modes of tuning were used: a) Shallow Tuning, b) Deep Tuning, c) Fine tuning. Shallow tuning means, only the last layer is kept trainable. In fine tuning, layer by layer is made trainable and kept rest of the layers frozen. In deep tuning, it is nothing but complete retraining of the model with this dataset (high time consuming).\n",
    "\n",
    "- In conclusion, the order of their performance: Deep Tuning > Fine Tuning > Shallow Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, GlobalMaxPooling2D, AveragePooling2D\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import shutil\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from imgaug import augmenters as imaug\n",
    "import skimage.io as skio\n",
    "\n",
    "from skimage.exposure import histogram\n",
    "from skimage import data, img_as_float\n",
    "from skimage import exposure\n",
    "\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint,CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------- Func to load Data---------------------------------------------------#\n",
    "\n",
    "def loadData(aug_path,m,n):\n",
    "    image = []\n",
    "    label = []\n",
    "    folders = os.listdir(aug_path)\n",
    "    for folder in folders:\n",
    "        folderpath = os.path.join(aug_path,folder)\n",
    "        if folder == 'MontgomerySet':\n",
    "            subfolders = os.listdir(folder)\n",
    "            for subfolder in subfolders:\n",
    "                print(subfolder)\n",
    "                if subfolder == 'CXR_png':\n",
    "                    subfolder_path = os.path.join(folderpath,subfolder)\n",
    "                    print(subfolder_path)\n",
    "                    fulldata = os.listdir(subfolder_path)\n",
    "                    for data in fulldata:\n",
    "                        if (data.endswith('.png')):\n",
    "                            datapath = os.path.join(subfolder_path,data)\n",
    "                            print(datapath)\n",
    "                            img = cv2.resize(cv2.imread(datapath),(m,n))\n",
    "                            im = img\n",
    "                            for style in aug_types:\n",
    "                                newimg = augmenter(im,style)\n",
    "                                save_path = datapath +\"-\"+ style +\".png\"\n",
    "                                cv2.imwrite(save_path,newimg)\n",
    "                                b,g,r = cv2.split(newimg)\n",
    "                                img_eq1 = exposure.equalize_hist(b)\n",
    "                                img_eq2 = exposure.equalize_hist(g)\n",
    "                                img_eq3 = exposure.equalize_hist(r)\n",
    "                                img_eq = np.zeros((m,n,3))\n",
    "                                img_eq[:,:,0] = img_eq1\n",
    "                                img_eq[:,:,1] = img_eq2\n",
    "                                img_eq[:,:,2] = img_eq3\n",
    "                                plt.imshow(img_eq)             \n",
    "                                image.append(img_eq)\n",
    "                                path_split = datapath.split(\"\\\\\")\n",
    "                                newpath = path_split[-1]\n",
    "                                label.append(newpath[12])\n",
    "    image = np.array(image)\n",
    "    label = np.array(label)\n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "ClinicalReadings\n",
      "CXR_png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0001_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0002_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0003_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0004_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0005_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0006_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0008_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0011_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0013_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0015_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0016_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0017_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0019_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0020_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0021_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0022_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0023_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0024_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0026_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0027_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0028_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0029_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0030_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0031_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0035_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0038_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0040_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0041_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0042_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0043_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0044_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0045_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0046_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0047_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0048_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0049_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0051_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0052_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0053_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0054_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0055_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0056_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0057_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0058_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0059_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0060_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0061_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0062_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0063_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0064_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0068_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0069_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0070_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0071_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0072_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0074_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0075_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0077_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0079_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0080_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0081_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0082_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0083_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0084_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0085_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0086_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0087_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0089_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0090_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0091_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0092_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0094_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0095_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0096_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0097_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0099_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0100_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0101_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0102_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0103_0.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0104_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0108_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0113_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0117_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0126_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0140_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0141_1.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0142_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0144_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0150_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0162_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0166_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0170_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0173_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0182_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0188_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0194_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0195_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0196_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0203_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0213_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0218_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0223_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0228_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0243_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0251_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0253_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0254_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0255_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0258_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0264_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0266_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0275_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0282_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0289_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0294_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0301_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0309_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0311_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0313_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0316_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0331_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0334_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0338_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0348_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0350_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0352_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0354_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0362_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0367_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0369_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0372_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0375_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0383_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0387_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0390_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0393_1.png\n",
      "D:\\CEN\\Sem 3 CEN\\Sowmya mam(Bio medical)\\tuberclosis\\MontgomerySet\\CXR_png\\MCUCXR_0399_1.png\n",
      "ManualMask\n",
      "NLM-MontgomeryCXRSet-ReadMe.pdf\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#------------------------------------------ Augumentation Types------------------------------------------------------------#\n",
    "\n",
    "aug_types = {\"Flip\",\"Crop\",\"Gaussian Blur\",\"Gaussian Noise\",\"Contrast\",\"Multiply\",\"Affine\",\"Scale\",\"Rotate\",\"Original\"}\n",
    "aug_path = 'D:\\\\CEN\\\\Sem 3 CEN\\\\Sowmya mam(Bio medical)\\\\tuberclosis'\n",
    "m = 224\n",
    "n = 224\n",
    "image,label = loadData(aug_path,m,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmenter(image,style):\n",
    "    if (style ==\"Flip\" ):\n",
    "        aug_img = imaug.Sequential([imaug.Fliplr(0.5)]).augment_image(image)         \n",
    "    elif(style == \"Crop\"):\n",
    "        aug_img = imaug.Sequential([imaug.Crop(percent = (0,0.1))]).augment_image(image)\n",
    "    elif(style == \"Gaussian Blur\"):    \n",
    "        aug_img = imaug.Sequential([imaug.GaussianBlur(sigma = (0,3.0))]).augment_image(image)\n",
    "    elif(style == \"Gaussian Noise\"):\n",
    "        aug_img = imaug.Sequential([imaug.AdditiveGaussianNoise(scale = (0.0,0.2))]).augment_image(image)\n",
    "    elif(style == \"Contrast\" ):\n",
    "        aug_img = imaug.Sequential([imaug.ContrastNormalization(0.5,per_channel = 0.5)]).augment_image(image)\n",
    "    elif(style == \"Multiply\" ):\n",
    "        aug_img = imaug.Sequential([imaug.Multiply((0.8,1.2))]).augment_image(image)\n",
    "    elif(style == \"Affine\" ):\n",
    "        aug_img = imaug.Sequential([imaug.Crop(percent = (0,0.1))]).augment_image(image)\n",
    "    elif(style == \"Scale\" ):\n",
    "        aug_img = imaug.Sequential([imaug.Affine(scale = {\"x\": (0.8,1.2),\"y\": (0.8,1.2)})]).augment_image(image)\n",
    "    elif(style == \"Rotate\" ):\n",
    "        aug_img = imaug.Sequential([imaug.Affine(rotate = (30))]).augment_image(image)\n",
    "    elif(style == \"Original\"):\n",
    "        aug_img = image\n",
    "\n",
    "    return aug_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '0' '0' ... '1' '1' '1'] \n",
      "\n",
      "(1380, 224, 224, 3)\n",
      "(1380,)\n"
     ]
    }
   ],
   "source": [
    "print(label,'\\n')\n",
    "print(image.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1242, 224, 224, 3)\n",
      "(1242,)\n",
      "(138, 224, 224, 3)\n",
      "(138,)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------Train-Test Split---------------------------------------------#\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(image, label, test_size=0.1, random_state = 30)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138, 2)\n"
     ]
    }
   ],
   "source": [
    "le=LabelEncoder()\n",
    "y11=le.fit_transform(y_test)\n",
    "y1_test = to_categorical(y11, 2)\n",
    "print(y1_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(993, 224, 224, 3)\n",
      "(993,)\n",
      "(249, 224, 224, 3)\n",
      "(249,)\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------Train-Valid Split-------------------------------------------------------#\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(X_train, y_train, test_size=0.2, random_state = 25)\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(valid_x.shape)\n",
    "print(valid_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(993, 2)\n",
      "(249, 2)\n"
     ]
    }
   ],
   "source": [
    "le1=LabelEncoder()\n",
    "le2=LabelEncoder()\n",
    "y1=le1.fit_transform(train_y)\n",
    "y2=le2.fit_transform(valid_y)\n",
    "y1_train = to_categorical(y1, 2)\n",
    "y1_valid = to_categorical(y2, 2)\n",
    "print(y1_train.shape)\n",
    "print(y1_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_pad (ZeroPadding2D)       (None, 225, 225, 3)  0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv2D)                  (None, 112, 112, 32) 864         Conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_Conv1 (BatchNormalization)   (None, 112, 112, 32) 128         Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_relu (ReLU)               (None, 112, 112, 32) 0           bn_Conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise (Depthw (None, 112, 112, 32) 288         Conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_BN (Bat (None, 112, 112, 32) 128         expanded_conv_depthwise[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_relu (R (None, 112, 112, 32) 0           expanded_conv_depthwise_BN[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project (Conv2D)  (None, 112, 112, 16) 512         expanded_conv_depthwise_relu[0][0\n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project_BN (Batch (None, 112, 112, 16) 64          expanded_conv_project[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand (Conv2D)         (None, 112, 112, 96) 1536        expanded_conv_project_BN[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_BN (BatchNormali (None, 112, 112, 96) 384         block_1_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_relu (ReLU)      (None, 112, 112, 96) 0           block_1_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_pad (ZeroPadding2D)     (None, 113, 113, 96) 0           block_1_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise (DepthwiseCon (None, 56, 56, 96)   864         block_1_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_BN (BatchNorm (None, 56, 56, 96)   384         block_1_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_relu (ReLU)   (None, 56, 56, 96)   0           block_1_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project (Conv2D)        (None, 56, 56, 24)   2304        block_1_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project_BN (BatchNormal (None, 56, 56, 24)   96          block_1_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand (Conv2D)         (None, 56, 56, 144)  3456        block_1_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_2_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_2_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise (DepthwiseCon (None, 56, 56, 144)  1296        block_2_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_BN (BatchNorm (None, 56, 56, 144)  576         block_2_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_relu (ReLU)   (None, 56, 56, 144)  0           block_2_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project (Conv2D)        (None, 56, 56, 24)   3456        block_2_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project_BN (BatchNormal (None, 56, 56, 24)   96          block_2_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_add (Add)               (None, 56, 56, 24)   0           block_1_project_BN[0][0]         \n",
      "                                                                 block_2_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand (Conv2D)         (None, 56, 56, 144)  3456        block_2_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_3_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_3_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_pad (ZeroPadding2D)     (None, 57, 57, 144)  0           block_3_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise (DepthwiseCon (None, 28, 28, 144)  1296        block_3_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_BN (BatchNorm (None, 28, 28, 144)  576         block_3_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_relu (ReLU)   (None, 28, 28, 144)  0           block_3_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project (Conv2D)        (None, 28, 28, 32)   4608        block_3_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project_BN (BatchNormal (None, 28, 28, 32)   128         block_3_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand (Conv2D)         (None, 28, 28, 192)  6144        block_3_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_4_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_4_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_4_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_4_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_4_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project (Conv2D)        (None, 28, 28, 32)   6144        block_4_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project_BN (BatchNormal (None, 28, 28, 32)   128         block_4_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_add (Add)               (None, 28, 28, 32)   0           block_3_project_BN[0][0]         \n",
      "                                                                 block_4_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand (Conv2D)         (None, 28, 28, 192)  6144        block_4_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_5_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_5_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_5_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_5_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_5_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project (Conv2D)        (None, 28, 28, 32)   6144        block_5_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project_BN (BatchNormal (None, 28, 28, 32)   128         block_5_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_5_add (Add)               (None, 28, 28, 32)   0           block_4_add[0][0]                \n",
      "                                                                 block_5_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand (Conv2D)         (None, 28, 28, 192)  6144        block_5_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_6_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_6_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_pad (ZeroPadding2D)     (None, 29, 29, 192)  0           block_6_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise (DepthwiseCon (None, 14, 14, 192)  1728        block_6_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_BN (BatchNorm (None, 14, 14, 192)  768         block_6_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_relu (ReLU)   (None, 14, 14, 192)  0           block_6_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project (Conv2D)        (None, 14, 14, 64)   12288       block_6_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project_BN (BatchNormal (None, 14, 14, 64)   256         block_6_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand (Conv2D)         (None, 14, 14, 384)  24576       block_6_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_7_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_7_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_7_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_7_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_7_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project (Conv2D)        (None, 14, 14, 64)   24576       block_7_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project_BN (BatchNormal (None, 14, 14, 64)   256         block_7_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_add (Add)               (None, 14, 14, 64)   0           block_6_project_BN[0][0]         \n",
      "                                                                 block_7_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand (Conv2D)         (None, 14, 14, 384)  24576       block_7_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_8_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_8_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_8_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_8_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_8_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project (Conv2D)        (None, 14, 14, 64)   24576       block_8_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project_BN (BatchNormal (None, 14, 14, 64)   256         block_8_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_8_add (Add)               (None, 14, 14, 64)   0           block_7_add[0][0]                \n",
      "                                                                 block_8_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand (Conv2D)         (None, 14, 14, 384)  24576       block_8_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_9_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_9_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_9_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_9_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_9_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project (Conv2D)        (None, 14, 14, 64)   24576       block_9_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project_BN (BatchNormal (None, 14, 14, 64)   256         block_9_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_9_add (Add)               (None, 14, 14, 64)   0           block_8_add[0][0]                \n",
      "                                                                 block_9_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand (Conv2D)        (None, 14, 14, 384)  24576       block_9_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_BN (BatchNormal (None, 14, 14, 384)  1536        block_10_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_relu (ReLU)     (None, 14, 14, 384)  0           block_10_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise (DepthwiseCo (None, 14, 14, 384)  3456        block_10_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_BN (BatchNor (None, 14, 14, 384)  1536        block_10_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           block_10_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project (Conv2D)       (None, 14, 14, 96)   36864       block_10_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project_BN (BatchNorma (None, 14, 14, 96)   384         block_10_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand (Conv2D)        (None, 14, 14, 576)  55296       block_10_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_11_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_11_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_11_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_11_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_11_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project (Conv2D)       (None, 14, 14, 96)   55296       block_11_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project_BN (BatchNorma (None, 14, 14, 96)   384         block_11_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_add (Add)              (None, 14, 14, 96)   0           block_10_project_BN[0][0]        \n",
      "                                                                 block_11_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand (Conv2D)        (None, 14, 14, 576)  55296       block_11_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_12_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_12_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_12_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_12_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_12_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project (Conv2D)       (None, 14, 14, 96)   55296       block_12_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project_BN (BatchNorma (None, 14, 14, 96)   384         block_12_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_12_add (Add)              (None, 14, 14, 96)   0           block_11_add[0][0]               \n",
      "                                                                 block_12_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand (Conv2D)        (None, 14, 14, 576)  55296       block_12_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_13_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_13_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_pad (ZeroPadding2D)    (None, 15, 15, 576)  0           block_13_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise (DepthwiseCo (None, 7, 7, 576)    5184        block_13_pad[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_BN (BatchNor (None, 7, 7, 576)    2304        block_13_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)    0           block_13_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project (Conv2D)       (None, 7, 7, 160)    92160       block_13_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project_BN (BatchNorma (None, 7, 7, 160)    640         block_13_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand (Conv2D)        (None, 7, 7, 960)    153600      block_13_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_14_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_14_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_14_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_14_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_14_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project (Conv2D)       (None, 7, 7, 160)    153600      block_14_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project_BN (BatchNorma (None, 7, 7, 160)    640         block_14_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_add (Add)              (None, 7, 7, 160)    0           block_13_project_BN[0][0]        \n",
      "                                                                 block_14_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand (Conv2D)        (None, 7, 7, 960)    153600      block_14_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_15_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_15_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_15_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_15_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_15_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project (Conv2D)       (None, 7, 7, 160)    153600      block_15_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project_BN (BatchNorma (None, 7, 7, 160)    640         block_15_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_15_add (Add)              (None, 7, 7, 160)    0           block_14_add[0][0]               \n",
      "                                                                 block_15_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand (Conv2D)        (None, 7, 7, 960)    153600      block_15_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_16_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_16_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_16_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_16_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_16_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project (Conv2D)       (None, 7, 7, 320)    307200      block_16_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project_BN (BatchNorma (None, 7, 7, 320)    1280        block_16_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1 (Conv2D)                 (None, 7, 7, 1280)   409600      block_16_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)   5120        Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_relu (ReLU)                 (None, 7, 7, 1280)   0           Conv_1_bn[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,257,984\n",
      "Trainable params: 2,223,872\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------- Pre-Trained MobileV2Net--------------------------------------------------#\n",
    "\n",
    "mobilev2 = MobileNetV2(input_shape=(m,n,3), alpha=1.0, include_top=False, weights='imagenet', input_tensor=None, pooling=None)\n",
    "mobilev2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Note:\n",
    "# ----------- If the last layer alone made trainable and rest layers is been freezed, then it is shallow tunning (in transfer learning approach)\n",
    "# ----------- If we start making layer by layer trainable from last, that is called fine tunning (in transfer learning approach)\n",
    "# ----------- Now fine tuning of the network is been done, in subsequent cells...\n",
    "\n",
    "for layer in frez1.layers[:-2]:\n",
    "    layer.trainable = False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_1.00_224 (Model) (None, 7, 7, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 2562      \n",
      "=================================================================\n",
      "Total params: 2,260,546\n",
      "Trainable params: 5,122\n",
      "Non-trainable params: 2,255,424\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(mobilev2)\n",
    "model1.add(GlobalAveragePooling2D())\n",
    "model1.add(Dense(2,activation='softmax'))\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.001, decay=0.001, momentum=0.9, nesterov=False)\n",
    "model1.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath = 'E:/CEN/Sem 3 CEN/Sowmya mam(Bio medical)/New check points/checkpoint1(2layers)/checkpoint-{epoch:02d}.hdf5',\n",
    "                             verbose=1,\n",
    "                             save_best_only = True,\n",
    "                             monitor='val_loss')\n",
    "\n",
    "csvLogger = CSVLogger(filename='E:/CEN/Sem 3 CEN/Sowmya mam(Bio medical)/New check points/checkpoint1(2layers)/logger.csv',\n",
    "                      append=True,\n",
    "                      separator=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0730 11:22:05.806278  8612 deprecation.py:323] From C:\\Users\\seshubabu\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 993 samples, validate on 249 samples\n",
      "Epoch 1/100\n",
      "993/993 [==============================] - 158s 159ms/step - loss: 0.7895 - acc: 0.4975 - val_loss: 1.4121 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.41215, saving model to E:/CEN/Sem 3 CEN/Sowmya mam(Bio medical)/New check points/checkpoint1(2layers)/checkpoint-01.hdf5\n",
      "Epoch 2/100\n",
      "993/993 [==============================] - 164s 165ms/step - loss: 0.7422 - acc: 0.5841 - val_loss: 1.3244 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.41215 to 1.32439, saving model to E:/CEN/Sem 3 CEN/Sowmya mam(Bio medical)/New check points/checkpoint1(2layers)/checkpoint-02.hdf5\n",
      "Epoch 3/100\n",
      "993/993 [==============================] - 260s 262ms/step - loss: 0.6793 - acc: 0.5952 - val_loss: 1.2794 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.32439 to 1.27943, saving model to E:/CEN/Sem 3 CEN/Sowmya mam(Bio medical)/New check points/checkpoint1(2layers)/checkpoint-03.hdf5\n",
      "Epoch 4/100\n",
      "993/993 [==============================] - 258s 260ms/step - loss: 0.6325 - acc: 0.6415 - val_loss: 1.4895 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.27943\n",
      "Epoch 5/100\n",
      "993/993 [==============================] - 258s 260ms/step - loss: 0.6073 - acc: 0.6798 - val_loss: 1.5486 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.27943\n",
      "Epoch 6/100\n",
      "993/993 [==============================] - 258s 260ms/step - loss: 0.5742 - acc: 0.7150 - val_loss: 1.5159 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.27943\n",
      "Epoch 7/100\n",
      "993/993 [==============================] - 261s 262ms/step - loss: 0.5539 - acc: 0.7170 - val_loss: 1.6850 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.27943\n",
      "Epoch 8/100\n",
      "993/993 [==============================] - 160s 162ms/step - loss: 0.5321 - acc: 0.7533 - val_loss: 1.6865 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.27943\n",
      "Epoch 9/100\n",
      "993/993 [==============================] - 158s 159ms/step - loss: 0.5160 - acc: 0.7513 - val_loss: 1.7470 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.27943\n",
      "Epoch 10/100\n",
      "993/993 [==============================] - 159s 160ms/step - loss: 0.5120 - acc: 0.7623 - val_loss: 1.7956 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.27943\n",
      "Epoch 11/100\n",
      "993/993 [==============================] - 159s 160ms/step - loss: 0.4868 - acc: 0.7905 - val_loss: 1.8045 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.27943\n",
      "Epoch 12/100\n",
      "993/993 [==============================] - 216s 217ms/step - loss: 0.4825 - acc: 0.8016 - val_loss: 1.8643 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.27943\n",
      "Epoch 13/100\n",
      "993/993 [==============================] - 260s 262ms/step - loss: 0.4654 - acc: 0.8036 - val_loss: 1.8865 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.27943\n",
      "Epoch 14/100\n",
      "993/993 [==============================] - 257s 259ms/step - loss: 0.4600 - acc: 0.8026 - val_loss: 1.9557 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.27943\n",
      "Epoch 15/100\n",
      "993/993 [==============================] - 259s 261ms/step - loss: 0.4508 - acc: 0.8087 - val_loss: 1.9483 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.27943\n",
      "Epoch 16/100\n",
      "993/993 [==============================] - 260s 262ms/step - loss: 0.4445 - acc: 0.8167 - val_loss: 1.9885 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.27943\n",
      "Epoch 17/100\n",
      "993/993 [==============================] - 260s 262ms/step - loss: 0.4399 - acc: 0.8157 - val_loss: 2.0362 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.27943\n",
      "Epoch 18/100\n",
      "993/993 [==============================] - 259s 261ms/step - loss: 0.4270 - acc: 0.8268 - val_loss: 2.0627 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.27943\n",
      "Epoch 19/100\n",
      "993/993 [==============================] - 260s 262ms/step - loss: 0.4165 - acc: 0.8409 - val_loss: 2.0085 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.27943\n",
      "Epoch 20/100\n",
      "993/993 [==============================] - 259s 261ms/step - loss: 0.4175 - acc: 0.8419 - val_loss: 2.0895 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.27943\n",
      "Epoch 21/100\n",
      "993/993 [==============================] - 260s 261ms/step - loss: 0.4084 - acc: 0.8348 - val_loss: 2.1072 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.27943\n",
      "Epoch 22/100\n",
      "993/993 [==============================] - 259s 260ms/step - loss: 0.4120 - acc: 0.8318 - val_loss: 2.1169 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.27943\n",
      "Epoch 23/100\n",
      "993/993 [==============================] - 255s 257ms/step - loss: 0.3988 - acc: 0.8348 - val_loss: 2.1786 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.27943\n",
      "Epoch 24/100\n",
      "993/993 [==============================] - 260s 262ms/step - loss: 0.3974 - acc: 0.8449 - val_loss: 2.1052 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.27943\n",
      "Epoch 25/100\n",
      "993/993 [==============================] - 258s 260ms/step - loss: 0.3849 - acc: 0.8439 - val_loss: 2.2211 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.27943\n",
      "Epoch 26/100\n",
      "993/993 [==============================] - 261s 263ms/step - loss: 0.3846 - acc: 0.8429 - val_loss: 2.2225 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.27943\n",
      "Epoch 27/100\n",
      "993/993 [==============================] - 260s 262ms/step - loss: 0.3816 - acc: 0.8600 - val_loss: 2.1433 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.27943\n",
      "Epoch 28/100\n",
      "993/993 [==============================] - 260s 262ms/step - loss: 0.3783 - acc: 0.8570 - val_loss: 2.2561 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.27943\n",
      "Epoch 29/100\n",
      "993/993 [==============================] - 255s 256ms/step - loss: 0.3715 - acc: 0.8540 - val_loss: 2.2563 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.27943\n",
      "Epoch 30/100\n",
      "993/993 [==============================] - 260s 262ms/step - loss: 0.3709 - acc: 0.8651 - val_loss: 2.2218 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.27943\n",
      "Epoch 31/100\n",
      "993/993 [==============================] - 259s 261ms/step - loss: 0.3615 - acc: 0.8761 - val_loss: 2.2843 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.27943\n",
      "Epoch 32/100\n",
      "993/993 [==============================] - 257s 259ms/step - loss: 0.3605 - acc: 0.8701 - val_loss: 2.2757 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.27943\n",
      "Epoch 33/100\n",
      "993/993 [==============================] - 256s 258ms/step - loss: 0.3600 - acc: 0.8711 - val_loss: 2.3349 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.27943\n",
      "Epoch 34/100\n",
      "993/993 [==============================] - 258s 260ms/step - loss: 0.3484 - acc: 0.8721 - val_loss: 2.2833 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.27943\n",
      "Epoch 35/100\n",
      "993/993 [==============================] - 259s 261ms/step - loss: 0.3504 - acc: 0.8701 - val_loss: 2.3088 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.27943\n",
      "Epoch 36/100\n",
      "993/993 [==============================] - 269s 271ms/step - loss: 0.3481 - acc: 0.8681 - val_loss: 2.3434 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.27943\n",
      "Epoch 37/100\n",
      "993/993 [==============================] - 261s 262ms/step - loss: 0.3419 - acc: 0.8771 - val_loss: 2.3315 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.27943\n",
      "Epoch 38/100\n",
      "993/993 [==============================] - 260s 262ms/step - loss: 0.3417 - acc: 0.8721 - val_loss: 2.3754 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.27943\n",
      "Epoch 39/100\n",
      "993/993 [==============================] - 264s 266ms/step - loss: 0.3369 - acc: 0.8862 - val_loss: 2.3353 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.27943\n",
      "Epoch 40/100\n",
      "993/993 [==============================] - 262s 264ms/step - loss: 0.3359 - acc: 0.8862 - val_loss: 2.3647 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.27943\n",
      "Epoch 41/100\n",
      "993/993 [==============================] - 267s 268ms/step - loss: 0.3324 - acc: 0.8933 - val_loss: 2.3888 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.27943\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "993/993 [==============================] - 272s 273ms/step - loss: 0.3347 - acc: 0.8751 - val_loss: 2.4008 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.27943\n",
      "Epoch 43/100\n",
      "993/993 [==============================] - 279s 281ms/step - loss: 0.3314 - acc: 0.8852 - val_loss: 2.3959 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.27943\n",
      "Epoch 44/100\n",
      "993/993 [==============================] - 261s 263ms/step - loss: 0.3278 - acc: 0.8872 - val_loss: 2.4423 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.27943\n",
      "Epoch 45/100\n",
      "993/993 [==============================] - 177s 178ms/step - loss: 0.3296 - acc: 0.8862 - val_loss: 2.4748 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.27943\n",
      "Epoch 46/100\n",
      "993/993 [==============================] - 176s 178ms/step - loss: 0.3226 - acc: 0.8943 - val_loss: 2.3995 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.27943\n",
      "Epoch 47/100\n",
      "993/993 [==============================] - 176s 177ms/step - loss: 0.3167 - acc: 0.8983 - val_loss: 2.4155 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.27943\n",
      "Epoch 48/100\n",
      "993/993 [==============================] - 176s 177ms/step - loss: 0.3195 - acc: 0.8892 - val_loss: 2.5003 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.27943\n",
      "Epoch 49/100\n",
      "993/993 [==============================] - 175s 176ms/step - loss: 0.3150 - acc: 0.8912 - val_loss: 2.4551 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.27943\n",
      "Epoch 50/100\n",
      "993/993 [==============================] - 177s 179ms/step - loss: 0.3195 - acc: 0.8892 - val_loss: 2.4705 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.27943\n",
      "Epoch 51/100\n",
      "993/993 [==============================] - 178s 179ms/step - loss: 0.3166 - acc: 0.8872 - val_loss: 2.4313 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.27943\n",
      "Epoch 52/100\n",
      "993/993 [==============================] - 179s 180ms/step - loss: 0.3104 - acc: 0.8983 - val_loss: 2.4750 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.27943\n",
      "Epoch 53/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.3127 - acc: 0.8872 - val_loss: 2.5787 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.27943\n",
      "Epoch 54/100\n",
      "993/993 [==============================] - 176s 177ms/step - loss: 0.3130 - acc: 0.8963 - val_loss: 2.4707 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.27943\n",
      "Epoch 55/100\n",
      "993/993 [==============================] - 180s 181ms/step - loss: 0.3070 - acc: 0.9053 - val_loss: 2.5167 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.27943\n",
      "Epoch 56/100\n",
      "993/993 [==============================] - 160s 161ms/step - loss: 0.2980 - acc: 0.9013 - val_loss: 2.5222 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.27943\n",
      "Epoch 57/100\n",
      "993/993 [==============================] - 163s 164ms/step - loss: 0.3038 - acc: 0.8983 - val_loss: 2.5389 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.27943\n",
      "Epoch 58/100\n",
      "993/993 [==============================] - 172s 173ms/step - loss: 0.3003 - acc: 0.9013 - val_loss: 2.4899 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.27943\n",
      "Epoch 59/100\n",
      "993/993 [==============================] - 160s 161ms/step - loss: 0.3000 - acc: 0.8973 - val_loss: 2.6063 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.27943\n",
      "Epoch 60/100\n",
      "993/993 [==============================] - 159s 161ms/step - loss: 0.2948 - acc: 0.9033 - val_loss: 2.5022 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.27943\n",
      "Epoch 61/100\n",
      "993/993 [==============================] - 159s 160ms/step - loss: 0.2968 - acc: 0.9124 - val_loss: 2.4852 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.27943\n",
      "Epoch 62/100\n",
      "993/993 [==============================] - 181s 182ms/step - loss: 0.2905 - acc: 0.9144 - val_loss: 2.5771 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.27943\n",
      "Epoch 63/100\n",
      "993/993 [==============================] - 184s 185ms/step - loss: 0.2894 - acc: 0.9134 - val_loss: 2.5961 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.27943\n",
      "Epoch 64/100\n",
      "993/993 [==============================] - 180s 181ms/step - loss: 0.2943 - acc: 0.9013 - val_loss: 2.5838 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.27943\n",
      "Epoch 65/100\n",
      "993/993 [==============================] - 170s 171ms/step - loss: 0.2854 - acc: 0.9033 - val_loss: 2.5343 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.27943\n",
      "Epoch 66/100\n",
      "993/993 [==============================] - 166s 167ms/step - loss: 0.2884 - acc: 0.9124 - val_loss: 2.5502 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.27943\n",
      "Epoch 67/100\n",
      "993/993 [==============================] - 164s 166ms/step - loss: 0.2903 - acc: 0.9053 - val_loss: 2.5602 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.27943\n",
      "Epoch 68/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.2880 - acc: 0.9134 - val_loss: 2.6125 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.27943\n",
      "Epoch 69/100\n",
      "993/993 [==============================] - 173s 174ms/step - loss: 0.2809 - acc: 0.9114 - val_loss: 2.5742 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.27943\n",
      "Epoch 70/100\n",
      "993/993 [==============================] - 172s 174ms/step - loss: 0.2818 - acc: 0.9164 - val_loss: 2.5601 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.27943\n",
      "Epoch 71/100\n",
      "993/993 [==============================] - 174s 176ms/step - loss: 0.2738 - acc: 0.9124 - val_loss: 2.5993 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.27943\n",
      "Epoch 72/100\n",
      "993/993 [==============================] - 181s 182ms/step - loss: 0.2840 - acc: 0.9074 - val_loss: 2.6020 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.27943\n",
      "Epoch 73/100\n",
      "993/993 [==============================] - 187s 188ms/step - loss: 0.2753 - acc: 0.9094 - val_loss: 2.6611 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.27943\n",
      "Epoch 74/100\n",
      "993/993 [==============================] - 174s 176ms/step - loss: 0.2776 - acc: 0.9033 - val_loss: 2.6183 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.27943\n",
      "Epoch 75/100\n",
      "993/993 [==============================] - 174s 175ms/step - loss: 0.2747 - acc: 0.9194 - val_loss: 2.5564 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.27943\n",
      "Epoch 76/100\n",
      "993/993 [==============================] - 172s 173ms/step - loss: 0.2842 - acc: 0.9084 - val_loss: 2.6085 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.27943\n",
      "Epoch 77/100\n",
      "993/993 [==============================] - 175s 176ms/step - loss: 0.2727 - acc: 0.9124 - val_loss: 2.6682 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.27943\n",
      "Epoch 78/100\n",
      "993/993 [==============================] - 226s 228ms/step - loss: 0.2746 - acc: 0.9134 - val_loss: 2.6557 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.27943\n",
      "Epoch 79/100\n",
      "993/993 [==============================] - 259s 261ms/step - loss: 0.2653 - acc: 0.9154 - val_loss: 2.6317 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.27943\n",
      "Epoch 80/100\n",
      "993/993 [==============================] - 269s 270ms/step - loss: 0.2683 - acc: 0.9104 - val_loss: 2.6434 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.27943\n",
      "Epoch 81/100\n",
      "993/993 [==============================] - 272s 274ms/step - loss: 0.2666 - acc: 0.9204 - val_loss: 2.6239 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.27943\n",
      "Epoch 82/100\n",
      "993/993 [==============================] - 264s 265ms/step - loss: 0.2658 - acc: 0.9194 - val_loss: 2.6365 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.27943\n",
      "Epoch 83/100\n",
      "993/993 [==============================] - 161s 162ms/step - loss: 0.2702 - acc: 0.9134 - val_loss: 2.6775 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.27943\n",
      "Epoch 84/100\n",
      "993/993 [==============================] - 160s 162ms/step - loss: 0.2691 - acc: 0.9134 - val_loss: 2.6913 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.27943\n",
      "Epoch 85/100\n",
      "993/993 [==============================] - 159s 160ms/step - loss: 0.2635 - acc: 0.9174 - val_loss: 2.6285 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.27943\n",
      "Epoch 86/100\n",
      "993/993 [==============================] - 159s 160ms/step - loss: 0.2639 - acc: 0.9215 - val_loss: 2.6595 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.27943\n",
      "Epoch 87/100\n",
      "993/993 [==============================] - 161s 162ms/step - loss: 0.2664 - acc: 0.9114 - val_loss: 2.6693 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.27943\n",
      "Epoch 88/100\n",
      "993/993 [==============================] - 159s 160ms/step - loss: 0.2611 - acc: 0.9215 - val_loss: 2.6990 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.27943\n",
      "Epoch 89/100\n",
      "993/993 [==============================] - 160s 162ms/step - loss: 0.2630 - acc: 0.9235 - val_loss: 2.6571 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.27943\n",
      "Epoch 90/100\n",
      "993/993 [==============================] - 160s 161ms/step - loss: 0.2572 - acc: 0.9245 - val_loss: 2.6755 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1.27943\n",
      "Epoch 91/100\n",
      "993/993 [==============================] - 160s 161ms/step - loss: 0.2595 - acc: 0.9215 - val_loss: 2.6871 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.27943\n",
      "Epoch 92/100\n",
      "993/993 [==============================] - 160s 161ms/step - loss: 0.2576 - acc: 0.9245 - val_loss: 2.6908 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.27943\n",
      "Epoch 93/100\n",
      "993/993 [==============================] - 159s 160ms/step - loss: 0.2588 - acc: 0.9215 - val_loss: 2.6798 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.27943\n",
      "Epoch 94/100\n",
      "993/993 [==============================] - 159s 160ms/step - loss: 0.2578 - acc: 0.9164 - val_loss: 2.6627 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.27943\n",
      "Epoch 95/100\n",
      "993/993 [==============================] - 160s 162ms/step - loss: 0.2548 - acc: 0.9144 - val_loss: 2.7129 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.27943\n",
      "Epoch 96/100\n",
      "993/993 [==============================] - 160s 161ms/step - loss: 0.2569 - acc: 0.9154 - val_loss: 2.7099 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.27943\n",
      "Epoch 97/100\n",
      "993/993 [==============================] - 159s 160ms/step - loss: 0.2546 - acc: 0.9204 - val_loss: 2.6952 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.27943\n",
      "Epoch 98/100\n",
      "993/993 [==============================] - 161s 162ms/step - loss: 0.2505 - acc: 0.9305 - val_loss: 2.6432 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.27943\n",
      "Epoch 99/100\n",
      "993/993 [==============================] - 159s 160ms/step - loss: 0.2504 - acc: 0.9275 - val_loss: 2.6688 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.27943\n",
      "Epoch 100/100\n",
      "993/993 [==============================] - 159s 161ms/step - loss: 0.2458 - acc: 0.9204 - val_loss: 2.7400 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.27943\n"
     ]
    }
   ],
   "source": [
    "model1.fit(train_x,y1_train,\n",
    "          batch_size=128,\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "          validation_data=(valid_x,y1_valid),\n",
    "         callbacks=[checkpoint,csvLogger])\n",
    "\n",
    "model1.save('E:/CEN/Sem 3 CEN/Sowmya mam(Bio medical)/New check points/checkpoint1(2layers)/model1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 21s 155ms/step\n"
     ]
    }
   ],
   "source": [
    "test_loss1 = model1.evaluate(X_test,y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.336960022000299, 0.5724637672521066]\n"
     ]
    }
   ],
   "source": [
    "print(test_loss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 23s 168ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions1 = model1.predict(X_test,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction1 = np.argmax(predictions1,axis=1)\n",
    "prediction1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Now last 3 layers is made trainable and rest layers is made freezed\n",
    "\n",
    "for layer in frez2.layers[:-3]:\n",
    "    layer.trainable = False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_1.00_224 (Model) (None, 7, 7, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 2562      \n",
      "=================================================================\n",
      "Total params: 2,260,546\n",
      "Trainable params: 414,722\n",
      "Non-trainable params: 1,845,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(frez2)\n",
    "model2.add(GlobalAveragePooling2D())\n",
    "model2.add(Dense(2,activation='softmax'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.001, decay=0.001, momentum=0.9, nesterov=False)\n",
    "model2.compile(loss=\"categorical_crossentropy\",optimizer=sgd,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath = 'E:/CEN/Sem 3 CEN/Sowmya mam(Bio medical)/New check points/checkpoint2(3layers)/checkpoint-{epoch:02d}.hdf5',\n",
    "                             verbose=1,\n",
    "                             save_best_only = True,\n",
    "                             monitor='val_loss')\n",
    "\n",
    "csvLogger = CSVLogger(filename='E:/CEN/Sem 3 CEN/Sowmya mam(Bio medical)/New check points/checkpoint2(3layers)/logger.csv',\n",
    "                      append=True,\n",
    "                      separator=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 993 samples, validate on 249 samples\n",
      "Epoch 1/100\n",
      "993/993 [==============================] - 262s 263ms/step - loss: 0.2526 - acc: 0.9255 - val_loss: 2.7603 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.76029, saving model to E:/CEN/Sem 3 CEN/Sowmya mam(Bio medical)/New check points/checkpoint2(3layers)/checkpoint-01.hdf5\n",
      "Epoch 2/100\n",
      "993/993 [==============================] - 266s 268ms/step - loss: 0.2487 - acc: 0.9225 - val_loss: 2.7091 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.76029 to 2.70907, saving model to E:/CEN/Sem 3 CEN/Sowmya mam(Bio medical)/New check points/checkpoint2(3layers)/checkpoint-02.hdf5\n",
      "Epoch 3/100\n",
      "993/993 [==============================] - 265s 266ms/step - loss: 0.2483 - acc: 0.9225 - val_loss: 2.6920 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.70907 to 2.69205, saving model to E:/CEN/Sem 3 CEN/Sowmya mam(Bio medical)/New check points/checkpoint2(3layers)/checkpoint-03.hdf5\n",
      "Epoch 4/100\n",
      "993/993 [==============================] - 263s 265ms/step - loss: 0.2485 - acc: 0.9285 - val_loss: 2.6956 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.69205\n",
      "Epoch 5/100\n",
      "993/993 [==============================] - 261s 262ms/step - loss: 0.2465 - acc: 0.9215 - val_loss: 2.7467 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.69205\n",
      "Epoch 6/100\n",
      "993/993 [==============================] - 261s 263ms/step - loss: 0.2538 - acc: 0.9225 - val_loss: 2.7330 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.69205\n",
      "Epoch 7/100\n",
      "993/993 [==============================] - 263s 265ms/step - loss: 0.2495 - acc: 0.9194 - val_loss: 2.7573 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.69205\n",
      "Epoch 8/100\n",
      "993/993 [==============================] - 263s 265ms/step - loss: 0.2461 - acc: 0.9245 - val_loss: 2.7046 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.69205\n",
      "Epoch 9/100\n",
      "993/993 [==============================] - 262s 264ms/step - loss: 0.2377 - acc: 0.9335 - val_loss: 2.7044 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.69205\n",
      "Epoch 10/100\n",
      "993/993 [==============================] - 262s 264ms/step - loss: 0.2465 - acc: 0.9204 - val_loss: 2.7339 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.69205\n",
      "Epoch 11/100\n",
      "993/993 [==============================] - 261s 263ms/step - loss: 0.2427 - acc: 0.9265 - val_loss: 2.7538 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.69205\n",
      "Epoch 12/100\n",
      "993/993 [==============================] - 263s 264ms/step - loss: 0.2461 - acc: 0.9235 - val_loss: 2.7556 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.69205\n",
      "Epoch 13/100\n",
      "993/993 [==============================] - 264s 265ms/step - loss: 0.2472 - acc: 0.9255 - val_loss: 2.7519 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.69205\n",
      "Epoch 14/100\n",
      "993/993 [==============================] - 264s 266ms/step - loss: 0.2407 - acc: 0.9315 - val_loss: 2.7151 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.69205\n",
      "Epoch 15/100\n",
      "993/993 [==============================] - 263s 265ms/step - loss: 0.2467 - acc: 0.9204 - val_loss: 2.7488 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.69205\n",
      "Epoch 16/100\n",
      "993/993 [==============================] - 263s 265ms/step - loss: 0.2406 - acc: 0.9315 - val_loss: 2.7459 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.69205\n",
      "Epoch 17/100\n",
      "993/993 [==============================] - 263s 265ms/step - loss: 0.2372 - acc: 0.9366 - val_loss: 2.7629 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.69205\n",
      "Epoch 18/100\n",
      "993/993 [==============================] - 262s 264ms/step - loss: 0.2385 - acc: 0.9255 - val_loss: 2.7673 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.69205\n",
      "Epoch 19/100\n",
      "993/993 [==============================] - 263s 265ms/step - loss: 0.2427 - acc: 0.9366 - val_loss: 2.7337 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.69205\n",
      "Epoch 20/100\n",
      "993/993 [==============================] - 263s 265ms/step - loss: 0.2397 - acc: 0.9345 - val_loss: 2.7519 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.69205\n",
      "Epoch 21/100\n",
      "993/993 [==============================] - 263s 264ms/step - loss: 0.2356 - acc: 0.9305 - val_loss: 2.7561 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.69205\n",
      "Epoch 22/100\n",
      "993/993 [==============================] - 261s 263ms/step - loss: 0.2395 - acc: 0.9285 - val_loss: 2.7655 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.69205\n",
      "Epoch 23/100\n",
      "993/993 [==============================] - 263s 265ms/step - loss: 0.2386 - acc: 0.9265 - val_loss: 2.7796 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.69205\n",
      "Epoch 24/100\n",
      "993/993 [==============================] - 262s 264ms/step - loss: 0.2313 - acc: 0.9315 - val_loss: 2.7932 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.69205\n",
      "Epoch 25/100\n",
      "993/993 [==============================] - 262s 264ms/step - loss: 0.2289 - acc: 0.9355 - val_loss: 2.7972 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.69205\n",
      "Epoch 26/100\n",
      "993/993 [==============================] - 262s 264ms/step - loss: 0.2306 - acc: 0.9386 - val_loss: 2.7630 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.69205\n",
      "Epoch 27/100\n",
      "993/993 [==============================] - 263s 265ms/step - loss: 0.2390 - acc: 0.9285 - val_loss: 2.7420 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.69205\n",
      "Epoch 28/100\n",
      "993/993 [==============================] - 264s 265ms/step - loss: 0.2327 - acc: 0.9315 - val_loss: 2.7757 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.69205\n",
      "Epoch 29/100\n",
      "993/993 [==============================] - 190s 192ms/step - loss: 0.2299 - acc: 0.9345 - val_loss: 2.8148 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.69205\n",
      "Epoch 30/100\n",
      "993/993 [==============================] - 166s 167ms/step - loss: 0.2344 - acc: 0.9245 - val_loss: 2.8173 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.69205\n",
      "Epoch 31/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.2333 - acc: 0.9275 - val_loss: 2.7379 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 2.69205\n",
      "Epoch 32/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.2311 - acc: 0.9315 - val_loss: 2.7828 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 2.69205\n",
      "Epoch 33/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.2308 - acc: 0.9335 - val_loss: 2.7757 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 2.69205\n",
      "Epoch 34/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.2299 - acc: 0.9345 - val_loss: 2.7926 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 2.69205\n",
      "Epoch 35/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.2267 - acc: 0.9366 - val_loss: 2.8093 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 2.69205\n",
      "Epoch 36/100\n",
      "993/993 [==============================] - 166s 167ms/step - loss: 0.2248 - acc: 0.9366 - val_loss: 2.7800 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 2.69205\n",
      "Epoch 37/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.2275 - acc: 0.9335 - val_loss: 2.7931 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 2.69205\n",
      "Epoch 38/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.2235 - acc: 0.9315 - val_loss: 2.8055 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 2.69205\n",
      "Epoch 39/100\n",
      "993/993 [==============================] - 166s 167ms/step - loss: 0.2227 - acc: 0.9355 - val_loss: 2.8098 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 2.69205\n",
      "Epoch 40/100\n",
      "993/993 [==============================] - 166s 168ms/step - loss: 0.2293 - acc: 0.9325 - val_loss: 2.8081 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 2.69205\n",
      "Epoch 41/100\n",
      "993/993 [==============================] - 166s 167ms/step - loss: 0.2340 - acc: 0.9285 - val_loss: 2.8338 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 2.69205\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "993/993 [==============================] - 167s 168ms/step - loss: 0.2376 - acc: 0.9335 - val_loss: 2.8013 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 2.69205\n",
      "Epoch 43/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.2250 - acc: 0.9396 - val_loss: 2.7601 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 2.69205\n",
      "Epoch 44/100\n",
      "993/993 [==============================] - 165s 166ms/step - loss: 0.2188 - acc: 0.9446 - val_loss: 2.8001 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 2.69205\n",
      "Epoch 45/100\n",
      "993/993 [==============================] - 187s 189ms/step - loss: 0.2208 - acc: 0.9406 - val_loss: 2.8622 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 2.69205\n",
      "Epoch 46/100\n",
      "993/993 [==============================] - 263s 265ms/step - loss: 0.2218 - acc: 0.9325 - val_loss: 2.8335 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 2.69205\n",
      "Epoch 47/100\n",
      "993/993 [==============================] - 262s 264ms/step - loss: 0.2186 - acc: 0.9436 - val_loss: 2.8129 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 2.69205\n",
      "Epoch 48/100\n",
      "993/993 [==============================] - 263s 265ms/step - loss: 0.2235 - acc: 0.9416 - val_loss: 2.8047 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 2.69205\n",
      "Epoch 49/100\n",
      "993/993 [==============================] - 263s 265ms/step - loss: 0.2212 - acc: 0.9366 - val_loss: 2.8292 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 2.69205\n",
      "Epoch 50/100\n",
      "993/993 [==============================] - 263s 265ms/step - loss: 0.2299 - acc: 0.9345 - val_loss: 2.8290 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 2.69205\n",
      "Epoch 51/100\n",
      "993/993 [==============================] - 262s 264ms/step - loss: 0.2178 - acc: 0.9416 - val_loss: 2.8387 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 2.69205\n",
      "Epoch 52/100\n",
      "993/993 [==============================] - 264s 266ms/step - loss: 0.2210 - acc: 0.9406 - val_loss: 2.8340 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 2.69205\n",
      "Epoch 53/100\n",
      "993/993 [==============================] - 270s 272ms/step - loss: 0.2189 - acc: 0.9426 - val_loss: 2.8292 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 2.69205\n",
      "Epoch 54/100\n",
      "993/993 [==============================] - 280s 282ms/step - loss: 0.2161 - acc: 0.9446 - val_loss: 2.8600 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 2.69205\n",
      "Epoch 55/100\n",
      "993/993 [==============================] - 276s 278ms/step - loss: 0.2216 - acc: 0.9376 - val_loss: 2.8231 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 2.69205\n",
      "Epoch 56/100\n",
      "993/993 [==============================] - 278s 280ms/step - loss: 0.2160 - acc: 0.9366 - val_loss: 2.8467 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 2.69205\n",
      "Epoch 57/100\n",
      "993/993 [==============================] - 280s 282ms/step - loss: 0.2129 - acc: 0.9426 - val_loss: 2.8231 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 2.69205\n",
      "Epoch 58/100\n",
      "993/993 [==============================] - 48749s 49s/step - loss: 0.2145 - acc: 0.9446 - val_loss: 2.8218 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 2.69205\n",
      "Epoch 59/100\n",
      "993/993 [==============================] - 184s 186ms/step - loss: 0.2171 - acc: 0.9386 - val_loss: 2.8570 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 2.69205\n",
      "Epoch 60/100\n",
      "993/993 [==============================] - 184s 186ms/step - loss: 0.2187 - acc: 0.9396 - val_loss: 2.8514 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 2.69205\n",
      "Epoch 61/100\n",
      "993/993 [==============================] - 184s 186ms/step - loss: 0.2140 - acc: 0.9406 - val_loss: 2.8561 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 2.69205\n",
      "Epoch 62/100\n",
      "993/993 [==============================] - 181s 182ms/step - loss: 0.2193 - acc: 0.9325 - val_loss: 2.8540 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 2.69205\n",
      "Epoch 63/100\n",
      "993/993 [==============================] - 180s 181ms/step - loss: 0.2125 - acc: 0.9436 - val_loss: 2.8659 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 2.69205\n",
      "Epoch 64/100\n",
      "993/993 [==============================] - 185s 187ms/step - loss: 0.2153 - acc: 0.9366 - val_loss: 2.8679 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 2.69205\n",
      "Epoch 65/100\n",
      "993/993 [==============================] - 180s 181ms/step - loss: 0.2158 - acc: 0.9426 - val_loss: 2.8484 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 2.69205\n",
      "Epoch 66/100\n",
      "993/993 [==============================] - 180s 181ms/step - loss: 0.2169 - acc: 0.9416 - val_loss: 2.8323 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 2.69205\n",
      "Epoch 67/100\n",
      "993/993 [==============================] - 179s 181ms/step - loss: 0.2114 - acc: 0.9426 - val_loss: 2.8654 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 2.69205\n",
      "Epoch 68/100\n",
      "993/993 [==============================] - 171s 172ms/step - loss: 0.2135 - acc: 0.9386 - val_loss: 2.8532 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 2.69205\n",
      "Epoch 69/100\n",
      "993/993 [==============================] - 166s 167ms/step - loss: 0.2084 - acc: 0.9426 - val_loss: 2.8795 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 2.69205\n",
      "Epoch 70/100\n",
      "993/993 [==============================] - 165s 167ms/step - loss: 0.2160 - acc: 0.9345 - val_loss: 2.8966 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 2.69205\n",
      "Epoch 71/100\n",
      "993/993 [==============================] - 166s 167ms/step - loss: 0.2121 - acc: 0.9426 - val_loss: 2.8435 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 2.69205\n",
      "Epoch 72/100\n",
      "993/993 [==============================] - 166s 167ms/step - loss: 0.2117 - acc: 0.9386 - val_loss: 2.8651 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 2.69205\n",
      "Epoch 73/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.2087 - acc: 0.9436 - val_loss: 2.8595 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 2.69205\n",
      "Epoch 74/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.2122 - acc: 0.9426 - val_loss: 2.8926 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 2.69205\n",
      "Epoch 75/100\n",
      "993/993 [==============================] - 166s 168ms/step - loss: 0.2081 - acc: 0.9396 - val_loss: 2.8814 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 2.69205\n",
      "Epoch 76/100\n",
      "993/993 [==============================] - 166s 167ms/step - loss: 0.2110 - acc: 0.9386 - val_loss: 2.8704 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 2.69205\n",
      "Epoch 77/100\n",
      "993/993 [==============================] - 166s 167ms/step - loss: 0.2112 - acc: 0.9355 - val_loss: 2.8512 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 2.69205\n",
      "Epoch 78/100\n",
      "993/993 [==============================] - 165s 166ms/step - loss: 0.2115 - acc: 0.9446 - val_loss: 2.8912 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 2.69205\n",
      "Epoch 79/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.2109 - acc: 0.9446 - val_loss: 2.8837 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 2.69205\n",
      "Epoch 80/100\n",
      "993/993 [==============================] - 166s 167ms/step - loss: 0.2045 - acc: 0.9386 - val_loss: 2.8705 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 2.69205\n",
      "Epoch 81/100\n",
      "993/993 [==============================] - 166s 167ms/step - loss: 0.2081 - acc: 0.9396 - val_loss: 2.8724 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 2.69205\n",
      "Epoch 82/100\n",
      "993/993 [==============================] - 166s 167ms/step - loss: 0.2047 - acc: 0.9436 - val_loss: 2.8676 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 2.69205\n",
      "Epoch 83/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.2053 - acc: 0.9466 - val_loss: 2.8826 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 2.69205\n",
      "Epoch 84/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.2053 - acc: 0.9496 - val_loss: 2.8871 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 2.69205\n",
      "Epoch 85/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.2104 - acc: 0.9366 - val_loss: 2.8803 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 2.69205\n",
      "Epoch 86/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.2049 - acc: 0.9436 - val_loss: 2.8875 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 2.69205\n",
      "Epoch 87/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.2065 - acc: 0.9416 - val_loss: 2.8638 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 2.69205\n",
      "Epoch 88/100\n",
      "993/993 [==============================] - 166s 167ms/step - loss: 0.2070 - acc: 0.9436 - val_loss: 2.8802 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 2.69205\n",
      "Epoch 89/100\n",
      "993/993 [==============================] - 166s 167ms/step - loss: 0.2014 - acc: 0.9476 - val_loss: 2.8969 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 2.69205\n",
      "Epoch 90/100\n",
      "993/993 [==============================] - 166s 168ms/step - loss: 0.2076 - acc: 0.9416 - val_loss: 2.8978 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 2.69205\n",
      "Epoch 91/100\n",
      "993/993 [==============================] - 166s 167ms/step - loss: 0.2066 - acc: 0.9426 - val_loss: 2.8743 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 2.69205\n",
      "Epoch 92/100\n",
      "993/993 [==============================] - 165s 167ms/step - loss: 0.2023 - acc: 0.9476 - val_loss: 2.8907 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 2.69205\n",
      "Epoch 93/100\n",
      "993/993 [==============================] - 165s 166ms/step - loss: 0.2042 - acc: 0.9426 - val_loss: 2.8849 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 2.69205\n",
      "Epoch 94/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.2082 - acc: 0.9376 - val_loss: 2.9089 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 2.69205\n",
      "Epoch 95/100\n",
      "993/993 [==============================] - 166s 167ms/step - loss: 0.2019 - acc: 0.9476 - val_loss: 2.9057 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 2.69205\n",
      "Epoch 96/100\n",
      "993/993 [==============================] - 165s 166ms/step - loss: 0.1972 - acc: 0.9436 - val_loss: 2.8790 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 2.69205\n",
      "Epoch 97/100\n",
      "993/993 [==============================] - 166s 167ms/step - loss: 0.2047 - acc: 0.9396 - val_loss: 2.8656 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 2.69205\n",
      "Epoch 98/100\n",
      "993/993 [==============================] - 165s 166ms/step - loss: 0.1962 - acc: 0.9547 - val_loss: 2.8902 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 2.69205\n",
      "Epoch 99/100\n",
      "993/993 [==============================] - 164s 166ms/step - loss: 0.2021 - acc: 0.9446 - val_loss: 2.8805 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 2.69205\n",
      "Epoch 100/100\n",
      "993/993 [==============================] - 165s 166ms/step - loss: 0.2036 - acc: 0.9396 - val_loss: 2.8990 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 2.69205\n"
     ]
    }
   ],
   "source": [
    "model2.fit(train_x,y1_train,\n",
    "          batch_size=128,\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "          validation_data=(valid_x,y1_valid),\n",
    "           callbacks=[checkpoint,csvLogger])\n",
    "\n",
    "model2.save('E:/CEN/Sem 3 CEN/Sowmya mam(Bio medical)/New check points/checkpoint2(3layers)/model1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 15s 111ms/step\n"
     ]
    }
   ],
   "source": [
    "test_loss2 = model2.evaluate(X_test,y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 15s 106ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions2 = model2.predict(X_test,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction2 = np.argmax(predictions2,axis=1)\n",
    "prediction2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Now last 4 layers is made trainable and rest layers is made freezed\n",
    "\n",
    "for layer in frez3.layers[:-4]:\n",
    "    layer.trainable = False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_1.00_224 (Model) (None, 7, 7, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_5 ( (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 2562      \n",
      "=================================================================\n",
      "Total params: 2,260,546\n",
      "Trainable params: 415,362\n",
      "Non-trainable params: 1,845,184\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(frez3)\n",
    "model3.add(GlobalAveragePooling2D())\n",
    "model3.add(Dense(2,activation='softmax'))\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.001, decay=0.001, momentum=0.9, nesterov=False)\n",
    "model3.compile(loss=\"categorical_crossentropy\",optimizer=sgd,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath = 'E:/CEN/Sem 3 CEN/Sowmya mam(Bio medical)/New check points/checkpoint3(4layers)/checkpoint-{epoch:02d}.hdf5',\n",
    "                             verbose=1,\n",
    "                             save_best_only = True,\n",
    "                             monitor='val_loss')\n",
    "\n",
    "csvLogger = CSVLogger(filename='E:/CEN/Sem 3 CEN/Sowmya mam(Bio medical)/New check points/checkpoint3(4layers)/logger.csv',\n",
    "                      append=True,\n",
    "                      separator=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 993 samples, validate on 249 samples\n",
      "Epoch 1/100\n",
      "993/993 [==============================] - 172s 174ms/step - loss: 0.7895 - acc: 0.5116 - val_loss: 1.2984 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.29838, saving model to E:/CEN/Sem 3 CEN/Sowmya mam(Bio medical)/New check points/checkpoint3(4layers)/checkpoint-01.hdf5\n",
      "Epoch 2/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.6795 - acc: 0.6224 - val_loss: 1.3145 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.29838\n",
      "Epoch 3/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.5712 - acc: 0.6959 - val_loss: 1.4035 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.29838\n",
      "Epoch 4/100\n",
      "993/993 [==============================] - 187s 188ms/step - loss: 0.4983 - acc: 0.7633 - val_loss: 1.7617 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.29838\n",
      "Epoch 5/100\n",
      "993/993 [==============================] - 179s 180ms/step - loss: 0.4448 - acc: 0.8117 - val_loss: 1.7523 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.29838\n",
      "Epoch 6/100\n",
      "993/993 [==============================] - 169s 171ms/step - loss: 0.4017 - acc: 0.8520 - val_loss: 1.9229 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.29838\n",
      "Epoch 7/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.3776 - acc: 0.8610 - val_loss: 1.9838 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.29838\n",
      "Epoch 8/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.3555 - acc: 0.8832 - val_loss: 2.0234 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.29838\n",
      "Epoch 9/100\n",
      "993/993 [==============================] - 171s 173ms/step - loss: 0.3335 - acc: 0.8922 - val_loss: 2.1146 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.29838\n",
      "Epoch 10/100\n",
      "993/993 [==============================] - 170s 171ms/step - loss: 0.3210 - acc: 0.8943 - val_loss: 2.2042 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.29838\n",
      "Epoch 11/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.3019 - acc: 0.9144 - val_loss: 2.1649 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.29838\n",
      "Epoch 12/100\n",
      "993/993 [==============================] - 179s 180ms/step - loss: 0.2872 - acc: 0.9063 - val_loss: 2.2891 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.29838\n",
      "Epoch 13/100\n",
      "993/993 [==============================] - 185s 186ms/step - loss: 0.2705 - acc: 0.9235 - val_loss: 2.3139 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.29838\n",
      "Epoch 14/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.2658 - acc: 0.9194 - val_loss: 2.3575 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.29838\n",
      "Epoch 15/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.2491 - acc: 0.9315 - val_loss: 2.3345 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.29838\n",
      "Epoch 16/100\n",
      "993/993 [==============================] - 168s 170ms/step - loss: 0.2441 - acc: 0.9335 - val_loss: 2.4222 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.29838\n",
      "Epoch 17/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.2442 - acc: 0.9386 - val_loss: 2.5225 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.29838\n",
      "Epoch 18/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.2317 - acc: 0.9345 - val_loss: 2.4703 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.29838\n",
      "Epoch 19/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.2218 - acc: 0.9547 - val_loss: 2.4678 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.29838\n",
      "Epoch 20/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.2187 - acc: 0.9446 - val_loss: 2.5970 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.29838\n",
      "Epoch 21/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.2098 - acc: 0.9517 - val_loss: 2.5880 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.29838\n",
      "Epoch 22/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.2043 - acc: 0.9537 - val_loss: 2.5758 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.29838\n",
      "Epoch 23/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.1998 - acc: 0.9567 - val_loss: 2.5863 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.29838\n",
      "Epoch 24/100\n",
      "993/993 [==============================] - 167s 169ms/step - loss: 0.1961 - acc: 0.9527 - val_loss: 2.6536 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.29838\n",
      "Epoch 25/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.1831 - acc: 0.9577 - val_loss: 2.6825 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.29838\n",
      "Epoch 26/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.1798 - acc: 0.9597 - val_loss: 2.6544 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.29838\n",
      "Epoch 27/100\n",
      "993/993 [==============================] - 168s 170ms/step - loss: 0.1762 - acc: 0.9648 - val_loss: 2.7384 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.29838\n",
      "Epoch 28/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.1688 - acc: 0.9698 - val_loss: 2.7088 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.29838\n",
      "Epoch 29/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.1657 - acc: 0.9718 - val_loss: 2.7302 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.29838\n",
      "Epoch 30/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.1658 - acc: 0.9658 - val_loss: 2.7574 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.29838\n",
      "Epoch 31/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.1561 - acc: 0.9718 - val_loss: 2.7451 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.29838\n",
      "Epoch 32/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.1567 - acc: 0.9678 - val_loss: 2.7300 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.29838\n",
      "Epoch 33/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.1510 - acc: 0.9738 - val_loss: 2.7808 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.29838\n",
      "Epoch 34/100\n",
      "993/993 [==============================] - 167s 169ms/step - loss: 0.1492 - acc: 0.9758 - val_loss: 2.8246 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.29838\n",
      "Epoch 35/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.1461 - acc: 0.9748 - val_loss: 2.8150 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.29838\n",
      "Epoch 36/100\n",
      "993/993 [==============================] - 167s 169ms/step - loss: 0.1377 - acc: 0.9789 - val_loss: 2.8267 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.29838\n",
      "Epoch 37/100\n",
      "993/993 [==============================] - 168s 170ms/step - loss: 0.1379 - acc: 0.9809 - val_loss: 2.8280 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.29838\n",
      "Epoch 38/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.1324 - acc: 0.9849 - val_loss: 2.8248 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.29838\n",
      "Epoch 39/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.1352 - acc: 0.9799 - val_loss: 2.8647 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.29838\n",
      "Epoch 40/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.1293 - acc: 0.9799 - val_loss: 2.8566 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.29838\n",
      "Epoch 41/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.1275 - acc: 0.9819 - val_loss: 2.8772 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.29838\n",
      "Epoch 42/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.1216 - acc: 0.9839 - val_loss: 2.8940 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.29838\n",
      "Epoch 43/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.1235 - acc: 0.9849 - val_loss: 2.9221 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.29838\n",
      "Epoch 44/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.1195 - acc: 0.9889 - val_loss: 2.9273 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.29838\n",
      "Epoch 45/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.1209 - acc: 0.9879 - val_loss: 2.9295 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.29838\n",
      "Epoch 46/100\n",
      "993/993 [==============================] - 167s 169ms/step - loss: 0.1187 - acc: 0.9889 - val_loss: 2.9405 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.29838\n",
      "Epoch 47/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.1105 - acc: 0.9909 - val_loss: 2.9324 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.29838\n",
      "Epoch 48/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.1084 - acc: 0.9950 - val_loss: 2.9397 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.29838\n",
      "Epoch 49/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.1109 - acc: 0.9909 - val_loss: 2.9683 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.29838\n",
      "Epoch 50/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.1072 - acc: 0.9879 - val_loss: 2.9765 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.29838\n",
      "Epoch 51/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.1052 - acc: 0.9889 - val_loss: 3.0000 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.29838\n",
      "Epoch 52/100\n",
      "993/993 [==============================] - 166s 168ms/step - loss: 0.1090 - acc: 0.9909 - val_loss: 3.0092 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.29838\n",
      "Epoch 53/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.1018 - acc: 0.9919 - val_loss: 3.0498 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.29838\n",
      "Epoch 54/100\n",
      "993/993 [==============================] - 167s 169ms/step - loss: 0.1054 - acc: 0.9879 - val_loss: 3.0546 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.29838\n",
      "Epoch 55/100\n",
      "993/993 [==============================] - 166s 168ms/step - loss: 0.0999 - acc: 0.9919 - val_loss: 3.0112 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.29838\n",
      "Epoch 56/100\n",
      "993/993 [==============================] - 166s 168ms/step - loss: 0.0952 - acc: 0.9940 - val_loss: 3.0213 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.29838\n",
      "Epoch 57/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.1024 - acc: 0.9930 - val_loss: 3.0312 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.29838\n",
      "Epoch 58/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.0977 - acc: 0.9930 - val_loss: 3.0484 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.29838\n",
      "Epoch 59/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.0946 - acc: 0.9940 - val_loss: 3.0576 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.29838\n",
      "Epoch 60/100\n",
      "993/993 [==============================] - 167s 169ms/step - loss: 0.0947 - acc: 0.9950 - val_loss: 3.0839 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.29838\n",
      "Epoch 61/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.0883 - acc: 0.9940 - val_loss: 3.0775 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.29838\n",
      "Epoch 62/100\n",
      "993/993 [==============================] - 167s 169ms/step - loss: 0.0872 - acc: 0.9960 - val_loss: 3.1089 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.29838\n",
      "Epoch 63/100\n",
      "993/993 [==============================] - 167s 169ms/step - loss: 0.0873 - acc: 0.9919 - val_loss: 3.1137 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.29838\n",
      "Epoch 64/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.0920 - acc: 0.9909 - val_loss: 3.0636 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.29838\n",
      "Epoch 65/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.0821 - acc: 0.9960 - val_loss: 3.1058 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.29838\n",
      "Epoch 66/100\n",
      "993/993 [==============================] - 166s 167ms/step - loss: 0.0831 - acc: 0.9970 - val_loss: 3.1205 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.29838\n",
      "Epoch 67/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.0804 - acc: 0.9980 - val_loss: 3.1526 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.29838\n",
      "Epoch 68/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.0815 - acc: 0.9970 - val_loss: 3.1736 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.29838\n",
      "Epoch 69/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.0774 - acc: 0.9980 - val_loss: 3.1685 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.29838\n",
      "Epoch 70/100\n",
      "993/993 [==============================] - 167s 169ms/step - loss: 0.0773 - acc: 0.9980 - val_loss: 3.1650 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.29838\n",
      "Epoch 71/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.0799 - acc: 0.9970 - val_loss: 3.1661 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.29838\n",
      "Epoch 72/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.0771 - acc: 0.9960 - val_loss: 3.1655 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.29838\n",
      "Epoch 73/100\n",
      "993/993 [==============================] - 167s 169ms/step - loss: 0.0781 - acc: 0.9970 - val_loss: 3.1832 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.29838\n",
      "Epoch 74/100\n",
      "993/993 [==============================] - 167s 169ms/step - loss: 0.0723 - acc: 0.9960 - val_loss: 3.1749 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.29838\n",
      "Epoch 75/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.0709 - acc: 0.9980 - val_loss: 3.1929 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.29838\n",
      "Epoch 76/100\n",
      "993/993 [==============================] - 167s 169ms/step - loss: 0.0745 - acc: 0.9950 - val_loss: 3.1857 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.29838\n",
      "Epoch 77/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.0773 - acc: 0.9970 - val_loss: 3.1665 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.29838\n",
      "Epoch 78/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.0696 - acc: 0.9990 - val_loss: 3.1718 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.29838\n",
      "Epoch 79/100\n",
      "993/993 [==============================] - 168s 170ms/step - loss: 0.0726 - acc: 0.9990 - val_loss: 3.1987 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.29838\n",
      "Epoch 80/100\n",
      "993/993 [==============================] - 169s 171ms/step - loss: 0.0705 - acc: 0.9960 - val_loss: 3.2057 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.29838\n",
      "Epoch 81/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.0648 - acc: 1.0000 - val_loss: 3.2083 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.29838\n",
      "Epoch 82/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.0688 - acc: 1.0000 - val_loss: 3.2335 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.29838\n",
      "Epoch 83/100\n",
      "993/993 [==============================] - 167s 169ms/step - loss: 0.0673 - acc: 0.9980 - val_loss: 3.2362 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.29838\n",
      "Epoch 84/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.0671 - acc: 0.9980 - val_loss: 3.2626 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.29838\n",
      "Epoch 85/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.0651 - acc: 0.9970 - val_loss: 3.2421 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.29838\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "993/993 [==============================] - 168s 169ms/step - loss: 0.0652 - acc: 0.9990 - val_loss: 3.2226 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.29838\n",
      "Epoch 87/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.0636 - acc: 0.9990 - val_loss: 3.2129 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.29838\n",
      "Epoch 88/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.0620 - acc: 0.9990 - val_loss: 3.2408 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.29838\n",
      "Epoch 89/100\n",
      "993/993 [==============================] - 166s 168ms/step - loss: 0.0624 - acc: 0.9990 - val_loss: 3.2246 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.29838\n",
      "Epoch 90/100\n",
      "993/993 [==============================] - 177s 179ms/step - loss: 0.0620 - acc: 0.9990 - val_loss: 3.2442 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1.29838\n",
      "Epoch 91/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.0617 - acc: 0.9990 - val_loss: 3.2452 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.29838\n",
      "Epoch 92/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.0640 - acc: 0.9970 - val_loss: 3.2436 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.29838\n",
      "Epoch 93/100\n",
      "993/993 [==============================] - 167s 169ms/step - loss: 0.0583 - acc: 0.9980 - val_loss: 3.2724 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.29838\n",
      "Epoch 94/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.0584 - acc: 0.9980 - val_loss: 3.2801 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.29838\n",
      "Epoch 95/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.0586 - acc: 0.9980 - val_loss: 3.2940 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.29838\n",
      "Epoch 96/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.0587 - acc: 1.0000 - val_loss: 3.2995 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.29838\n",
      "Epoch 97/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.0601 - acc: 0.9970 - val_loss: 3.2928 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.29838\n",
      "Epoch 98/100\n",
      "993/993 [==============================] - 167s 169ms/step - loss: 0.0563 - acc: 0.9990 - val_loss: 3.2828 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.29838\n",
      "Epoch 99/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.0601 - acc: 1.0000 - val_loss: 3.2755 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.29838\n",
      "Epoch 100/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0532 - acc: 1.0000 - val_loss: 3.2941 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.29838\n"
     ]
    }
   ],
   "source": [
    "model3.fit(train_x,y1_train,\n",
    "          batch_size=128,\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "          validation_data=(valid_x,y1_valid),\n",
    "           callbacks=[checkpoint,csvLogger])\n",
    "\n",
    "model3.save('E:/CEN/Sem 3 CEN/Sowmya mam(Bio medical)/New check points/checkpoint3(4layers)/model3.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 22s 156ms/step\n"
     ]
    }
   ],
   "source": [
    "test_loss3 = model3.evaluate(X_test,y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 27s 197ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions3 = model3.predict(X_test,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction3 = np.argmax(predictions3,axis=1)\n",
    "prediction3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Now last 5 layers is made trainable and rest layers is made freezed\n",
    "\n",
    "for layer in frez4.layers[:-5]:\n",
    "    layer.trainable = False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_1.00_224 (Model) (None, 7, 7, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_8 ( (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 2562      \n",
      "=================================================================\n",
      "Total params: 2,260,546\n",
      "Trainable params: 722,562\n",
      "Non-trainable params: 1,537,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(frez4)\n",
    "model4.add(GlobalAveragePooling2D())\n",
    "model4.add(Dense(2,activation='softmax'))\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.001, decay=0.001, momentum=0.9, nesterov=False)\n",
    "model4.compile(loss=\"categorical_crossentropy\",optimizer=sgd,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath = 'E:/CEN/Sem 3 CEN/Sowmya mam(Bio medical)/New check points/checkpoint4(5layers)/checkpoint-{epoch:02d}.hdf5',\n",
    "                             verbose=1,\n",
    "                             save_best_only = True,\n",
    "                             monitor='val_loss')\n",
    "\n",
    "csvLogger = CSVLogger(filename='E:/CEN/Sem 3 CEN/Sowmya mam(Bio medical)/New check points/checkpoint4(5layers)/logger.csv',\n",
    "                      append=True,\n",
    "                      separator=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 993 samples, validate on 249 samples\n",
      "Epoch 1/100\n",
      "993/993 [==============================] - 291s 293ms/step - loss: 0.7625 - acc: 0.5438 - val_loss: 0.7778 - val_acc: 0.5141\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.77782, saving model to E:/CEN/Sem 3 CEN/Sowmya mam(Bio medical)/New check points/checkpoint4(5layers)/checkpoint-01.hdf5\n",
      "Epoch 2/100\n",
      "993/993 [==============================] - 269s 271ms/step - loss: 0.6228 - acc: 0.6556 - val_loss: 0.7578 - val_acc: 0.5542\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.77782 to 0.75781, saving model to E:/CEN/Sem 3 CEN/Sowmya mam(Bio medical)/New check points/checkpoint4(5layers)/checkpoint-02.hdf5\n",
      "Epoch 3/100\n",
      "993/993 [==============================] - 269s 271ms/step - loss: 0.5123 - acc: 0.7523 - val_loss: 0.7644 - val_acc: 0.5904\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.75781\n",
      "Epoch 4/100\n",
      "993/993 [==============================] - 266s 268ms/step - loss: 0.4283 - acc: 0.8157 - val_loss: 1.0384 - val_acc: 0.5622\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.75781\n",
      "Epoch 5/100\n",
      "993/993 [==============================] - 223s 224ms/step - loss: 0.3770 - acc: 0.8580 - val_loss: 1.1067 - val_acc: 0.5622\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.75781\n",
      "Epoch 6/100\n",
      "993/993 [==============================] - 190s 191ms/step - loss: 0.3362 - acc: 0.8761 - val_loss: 1.1752 - val_acc: 0.5542\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.75781\n",
      "Epoch 7/100\n",
      "993/993 [==============================] - 177s 178ms/step - loss: 0.3063 - acc: 0.8892 - val_loss: 1.3621 - val_acc: 0.5462\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.75781\n",
      "Epoch 8/100\n",
      "993/993 [==============================] - 177s 178ms/step - loss: 0.2830 - acc: 0.9063 - val_loss: 1.4363 - val_acc: 0.5462\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.75781\n",
      "Epoch 9/100\n",
      "993/993 [==============================] - 178s 179ms/step - loss: 0.2679 - acc: 0.9134 - val_loss: 1.4931 - val_acc: 0.5462\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.75781\n",
      "Epoch 10/100\n",
      "993/993 [==============================] - 177s 179ms/step - loss: 0.2448 - acc: 0.9315 - val_loss: 1.5858 - val_acc: 0.5462\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.75781\n",
      "Epoch 11/100\n",
      "993/993 [==============================] - 172s 173ms/step - loss: 0.2280 - acc: 0.9436 - val_loss: 1.6351 - val_acc: 0.5422\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.75781\n",
      "Epoch 12/100\n",
      "993/993 [==============================] - 174s 175ms/step - loss: 0.2201 - acc: 0.9486 - val_loss: 1.6333 - val_acc: 0.5422\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.75781\n",
      "Epoch 13/100\n",
      "993/993 [==============================] - 175s 177ms/step - loss: 0.2027 - acc: 0.9517 - val_loss: 1.7811 - val_acc: 0.5382\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.75781\n",
      "Epoch 14/100\n",
      "993/993 [==============================] - 171s 172ms/step - loss: 0.1933 - acc: 0.9607 - val_loss: 1.8327 - val_acc: 0.5382\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.75781\n",
      "Epoch 15/100\n",
      "993/993 [==============================] - 172s 173ms/step - loss: 0.1869 - acc: 0.9627 - val_loss: 1.8570 - val_acc: 0.5382\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.75781\n",
      "Epoch 16/100\n",
      "993/993 [==============================] - 170s 172ms/step - loss: 0.1756 - acc: 0.9627 - val_loss: 1.9211 - val_acc: 0.5382\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.75781\n",
      "Epoch 17/100\n",
      "993/993 [==============================] - 173s 174ms/step - loss: 0.1673 - acc: 0.9627 - val_loss: 1.9452 - val_acc: 0.5382\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.75781\n",
      "Epoch 18/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.1588 - acc: 0.9698 - val_loss: 2.0014 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.75781\n",
      "Epoch 19/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.1547 - acc: 0.9688 - val_loss: 2.0799 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.75781\n",
      "Epoch 20/100\n",
      "993/993 [==============================] - 172s 173ms/step - loss: 0.1460 - acc: 0.9738 - val_loss: 2.0973 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.75781\n",
      "Epoch 21/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.1422 - acc: 0.9758 - val_loss: 2.1186 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.75781\n",
      "Epoch 22/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.1367 - acc: 0.9768 - val_loss: 2.1956 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.75781\n",
      "Epoch 23/100\n",
      "993/993 [==============================] - 170s 171ms/step - loss: 0.1315 - acc: 0.9778 - val_loss: 2.2071 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.75781\n",
      "Epoch 24/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.1260 - acc: 0.9778 - val_loss: 2.2371 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.75781\n",
      "Epoch 25/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.1212 - acc: 0.9839 - val_loss: 2.2830 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.75781\n",
      "Epoch 26/100\n",
      "993/993 [==============================] - 167s 169ms/step - loss: 0.1164 - acc: 0.9859 - val_loss: 2.3202 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.75781\n",
      "Epoch 27/100\n",
      "993/993 [==============================] - 171s 172ms/step - loss: 0.1117 - acc: 0.9859 - val_loss: 2.3296 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.75781\n",
      "Epoch 28/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.1143 - acc: 0.9839 - val_loss: 2.3452 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.75781\n",
      "Epoch 29/100\n",
      "993/993 [==============================] - 169s 171ms/step - loss: 0.1053 - acc: 0.9899 - val_loss: 2.4030 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.75781\n",
      "Epoch 30/100\n",
      "993/993 [==============================] - 170s 171ms/step - loss: 0.1074 - acc: 0.9869 - val_loss: 2.4545 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.75781\n",
      "Epoch 31/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.1016 - acc: 0.9909 - val_loss: 2.4961 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.75781\n",
      "Epoch 32/100\n",
      "993/993 [==============================] - 173s 174ms/step - loss: 0.0936 - acc: 0.9930 - val_loss: 2.5271 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.75781\n",
      "Epoch 33/100\n",
      "993/993 [==============================] - 169s 171ms/step - loss: 0.0929 - acc: 0.9919 - val_loss: 2.5442 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.75781\n",
      "Epoch 34/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0930 - acc: 0.9919 - val_loss: 2.5555 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.75781\n",
      "Epoch 35/100\n",
      "993/993 [==============================] - 172s 173ms/step - loss: 0.0872 - acc: 0.9930 - val_loss: 2.5900 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.75781\n",
      "Epoch 36/100\n",
      "993/993 [==============================] - 170s 171ms/step - loss: 0.0859 - acc: 0.9940 - val_loss: 2.6127 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.75781\n",
      "Epoch 37/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0873 - acc: 0.9889 - val_loss: 2.6491 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.75781\n",
      "Epoch 38/100\n",
      "993/993 [==============================] - 170s 171ms/step - loss: 0.0785 - acc: 0.9950 - val_loss: 2.6810 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.75781\n",
      "Epoch 39/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0801 - acc: 0.9940 - val_loss: 2.7030 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.75781\n",
      "Epoch 40/100\n",
      "993/993 [==============================] - 169s 171ms/step - loss: 0.0768 - acc: 0.9970 - val_loss: 2.7197 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.75781\n",
      "Epoch 41/100\n",
      "993/993 [==============================] - 171s 172ms/step - loss: 0.0734 - acc: 0.9980 - val_loss: 2.7293 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.75781\n",
      "Epoch 42/100\n",
      "993/993 [==============================] - 172s 173ms/step - loss: 0.0734 - acc: 0.9970 - val_loss: 2.7635 - val_acc: 0.5301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00042: val_loss did not improve from 0.75781\n",
      "Epoch 43/100\n",
      "993/993 [==============================] - 168s 170ms/step - loss: 0.0710 - acc: 0.9970 - val_loss: 2.7737 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.75781\n",
      "Epoch 44/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0717 - acc: 0.9980 - val_loss: 2.8009 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.75781\n",
      "Epoch 45/100\n",
      "993/993 [==============================] - 168s 170ms/step - loss: 0.0686 - acc: 0.9960 - val_loss: 2.8189 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.75781\n",
      "Epoch 46/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0674 - acc: 0.9940 - val_loss: 2.7983 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.75781\n",
      "Epoch 47/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0659 - acc: 0.9960 - val_loss: 2.8255 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.75781\n",
      "Epoch 48/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0604 - acc: 0.9970 - val_loss: 2.8257 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.75781\n",
      "Epoch 49/100\n",
      "993/993 [==============================] - 169s 171ms/step - loss: 0.0615 - acc: 0.9960 - val_loss: 2.8489 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.75781\n",
      "Epoch 50/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0584 - acc: 0.9990 - val_loss: 2.8629 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.75781\n",
      "Epoch 51/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0601 - acc: 0.9970 - val_loss: 2.8990 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.75781\n",
      "Epoch 52/100\n",
      "993/993 [==============================] - 171s 172ms/step - loss: 0.0575 - acc: 0.9990 - val_loss: 2.9408 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.75781\n",
      "Epoch 53/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0549 - acc: 0.9990 - val_loss: 2.9767 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.75781\n",
      "Epoch 54/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.0548 - acc: 0.9990 - val_loss: 2.9697 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.75781\n",
      "Epoch 55/100\n",
      "993/993 [==============================] - 171s 172ms/step - loss: 0.0573 - acc: 0.9980 - val_loss: 2.9778 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.75781\n",
      "Epoch 56/100\n",
      "993/993 [==============================] - 179s 180ms/step - loss: 0.0526 - acc: 0.9980 - val_loss: 2.9925 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.75781\n",
      "Epoch 57/100\n",
      "993/993 [==============================] - 185s 186ms/step - loss: 0.0523 - acc: 0.9990 - val_loss: 3.0192 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.75781\n",
      "Epoch 58/100\n",
      "993/993 [==============================] - 183s 184ms/step - loss: 0.0502 - acc: 1.0000 - val_loss: 3.0214 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.75781\n",
      "Epoch 59/100\n",
      "993/993 [==============================] - 171s 172ms/step - loss: 0.0476 - acc: 1.0000 - val_loss: 3.0337 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.75781\n",
      "Epoch 60/100\n",
      "993/993 [==============================] - 167s 168ms/step - loss: 0.0515 - acc: 0.9980 - val_loss: 3.0353 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.75781\n",
      "Epoch 61/100\n",
      "993/993 [==============================] - 170s 171ms/step - loss: 0.0471 - acc: 1.0000 - val_loss: 3.0323 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.75781\n",
      "Epoch 62/100\n",
      "993/993 [==============================] - 170s 171ms/step - loss: 0.0507 - acc: 0.9980 - val_loss: 3.0308 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.75781\n",
      "Epoch 63/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0461 - acc: 0.9990 - val_loss: 3.0378 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.75781\n",
      "Epoch 64/100\n",
      "993/993 [==============================] - 196s 197ms/step - loss: 0.0473 - acc: 0.9990 - val_loss: 3.0345 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.75781\n",
      "Epoch 65/100\n",
      "993/993 [==============================] - 171s 172ms/step - loss: 0.0453 - acc: 0.9980 - val_loss: 3.0432 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.75781\n",
      "Epoch 66/100\n",
      "993/993 [==============================] - 174s 175ms/step - loss: 0.0449 - acc: 0.9990 - val_loss: 3.0598 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.75781\n",
      "Epoch 67/100\n",
      "993/993 [==============================] - 171s 173ms/step - loss: 0.0436 - acc: 0.9990 - val_loss: 3.0816 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.75781\n",
      "Epoch 68/100\n",
      "993/993 [==============================] - 171s 172ms/step - loss: 0.0408 - acc: 0.9990 - val_loss: 3.0851 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.75781\n",
      "Epoch 69/100\n",
      "993/993 [==============================] - 189s 190ms/step - loss: 0.0437 - acc: 0.9980 - val_loss: 3.0771 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.75781\n",
      "Epoch 70/100\n",
      "993/993 [==============================] - 190s 191ms/step - loss: 0.0465 - acc: 0.9980 - val_loss: 3.0986 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.75781\n",
      "Epoch 71/100\n",
      "993/993 [==============================] - 184s 185ms/step - loss: 0.0410 - acc: 1.0000 - val_loss: 3.1152 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.75781\n",
      "Epoch 72/100\n",
      "993/993 [==============================] - 171s 172ms/step - loss: 0.0429 - acc: 0.9990 - val_loss: 3.1153 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.75781\n",
      "Epoch 73/100\n",
      "993/993 [==============================] - 170s 171ms/step - loss: 0.0403 - acc: 1.0000 - val_loss: 3.1267 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.75781\n",
      "Epoch 74/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0395 - acc: 0.9990 - val_loss: 3.1517 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.75781\n",
      "Epoch 75/100\n",
      "993/993 [==============================] - 168s 170ms/step - loss: 0.0383 - acc: 1.0000 - val_loss: 3.1592 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.75781\n",
      "Epoch 76/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0381 - acc: 0.9990 - val_loss: 3.1569 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.75781\n",
      "Epoch 77/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.0381 - acc: 1.0000 - val_loss: 3.1682 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.75781\n",
      "Epoch 78/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0390 - acc: 0.9980 - val_loss: 3.1737 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.75781\n",
      "Epoch 79/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0348 - acc: 1.0000 - val_loss: 3.1696 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.75781\n",
      "Epoch 80/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0370 - acc: 1.0000 - val_loss: 3.1798 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.75781\n",
      "Epoch 81/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0340 - acc: 1.0000 - val_loss: 3.1845 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.75781\n",
      "Epoch 82/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0365 - acc: 1.0000 - val_loss: 3.1823 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.75781\n",
      "Epoch 83/100\n",
      "993/993 [==============================] - 171s 172ms/step - loss: 0.0333 - acc: 1.0000 - val_loss: 3.1965 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.75781\n",
      "Epoch 84/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.0318 - acc: 1.0000 - val_loss: 3.2047 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.75781\n",
      "Epoch 85/100\n",
      "993/993 [==============================] - 168s 170ms/step - loss: 0.0340 - acc: 1.0000 - val_loss: 3.2049 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.75781\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "993/993 [==============================] - 168s 170ms/step - loss: 0.0337 - acc: 1.0000 - val_loss: 3.2127 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.75781\n",
      "Epoch 87/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.0327 - acc: 1.0000 - val_loss: 3.2269 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.75781\n",
      "Epoch 88/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0341 - acc: 1.0000 - val_loss: 3.2358 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.75781\n",
      "Epoch 89/100\n",
      "993/993 [==============================] - 168s 170ms/step - loss: 0.0349 - acc: 1.0000 - val_loss: 3.2389 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.75781\n",
      "Epoch 90/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0336 - acc: 1.0000 - val_loss: 3.2474 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.75781\n",
      "Epoch 91/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.0296 - acc: 1.0000 - val_loss: 3.2351 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.75781\n",
      "Epoch 92/100\n",
      "993/993 [==============================] - 170s 171ms/step - loss: 0.0303 - acc: 1.0000 - val_loss: 3.2368 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.75781\n",
      "Epoch 93/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0293 - acc: 1.0000 - val_loss: 3.2410 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.75781\n",
      "Epoch 94/100\n",
      "993/993 [==============================] - 171s 172ms/step - loss: 0.0315 - acc: 1.0000 - val_loss: 3.2494 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.75781\n",
      "Epoch 95/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0272 - acc: 1.0000 - val_loss: 3.2523 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.75781\n",
      "Epoch 96/100\n",
      "993/993 [==============================] - 168s 170ms/step - loss: 0.0278 - acc: 1.0000 - val_loss: 3.2593 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.75781\n",
      "Epoch 97/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.0318 - acc: 1.0000 - val_loss: 3.2671 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.75781\n",
      "Epoch 98/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0283 - acc: 1.0000 - val_loss: 3.2782 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.75781\n",
      "Epoch 99/100\n",
      "993/993 [==============================] - 169s 170ms/step - loss: 0.0282 - acc: 1.0000 - val_loss: 3.2899 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.75781\n",
      "Epoch 100/100\n",
      "993/993 [==============================] - 168s 169ms/step - loss: 0.0278 - acc: 1.0000 - val_loss: 3.2950 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.75781\n"
     ]
    }
   ],
   "source": [
    "model4.fit(train_x,y1_train,\n",
    "          batch_size=128,\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "          validation_data=(valid_x,y1_valid),\n",
    "           callbacks=[checkpoint,csvLogger])\n",
    "\n",
    "model4.save('E:/CEN/Sem 3 CEN/Sowmya mam(Bio medical)/New check points/checkpoint4(5layers)/model4.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 23s 165ms/step\n"
     ]
    }
   ],
   "source": [
    "test_loss4 = model4.evaluate(X_test,y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 30s 215ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions4 = model4.predict(X_test,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction4 = np.argmax(predictions4,axis=1)\n",
    "prediction4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
